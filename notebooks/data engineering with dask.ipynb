{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🛠️ M5 Forecasting Data Preprocessing and Feature Engineering\n",
    "\n",
    "This notebook focuses on efficiently preparing the **M5 Forecasting** dataset for further analysis and modeling, primarily using **Dask** for handling large data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. 📦 Import Required Libraries\n",
    "- Imported Dask, NumPy, Pandas, OS, and JSON libraries.\n",
    "- Note: All necessary libraries should be installed from the provided `requirements.txt`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. 📂 Create Output Directory\n",
    "- Created a `./data` folder if it doesn't already exist to store processed datasets.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 3. 🛒 Sales Data Preprocessing\n",
    "- Read `sales_train_validation.csv` using Dask.\n",
    "- **Truncated** the dataset to only include `HOBBIES` category products due to limited compute resources.\n",
    "- **Repartitioned** the dataset based on `item_id` for better parallelism.\n",
    "- **Melted** the data:\n",
    "  - Converted it from wide format (columns `d_1`, `d_2`, ...) to long format (one row per item-day pair).\n",
    "- Extracted day numbers (e.g., `d_1` → `1`) from the `day` column.\n",
    "- **Sorted** data by `id` and `day`.\n",
    "- Saved the processed sales data in **Parquet** format for faster loading.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. 📈 Load Datasets\n",
    "- Loaded the following datasets:\n",
    "  - **Sell prices**: `sell_prices.csv`\n",
    "  - **Calendar**: `calendar.csv` (handled categorical columns and missing values properly)\n",
    "  - **Sales**: Loaded preprocessed sales data from the saved parquet files.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. 🧹 Handling Missing Values\n",
    "- Created a utility function `handle_missing_values(df)`:\n",
    "  - Generated a missing value report (percentage of nulls per column).\n",
    "  - Imputed missing values using **forward fill** (`ffill`) followed by **backward fill** (`bfill`).\n",
    "- Applied this function across **sales**, **prices**, and **calendar** datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. 🛠️ Memory Management\n",
    "- Repartitioned the sales dataset for better memory management and processing efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. 🧪 Feature Engineering\n",
    "- **Sorted** sales data by `id` and `day` again to ensure correct sequencing.\n",
    "- **Set `id` as the index** (ensuring a sorted index for partition operations).\n",
    "- Created **lag features**:\n",
    "  - Added lagged sales columns for **1**, **7**, and **28** days.\n",
    "  - These features will help capture sales patterns and temporal dependencies during modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### ✅ Output:\n",
    "- Preprocessed sales data with lag features ready for modeling.\n",
    "- Missing values handled across all datasets.\n",
    "- All transformations are optimized for large-scale data handling using Dask.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "#Note: Install all libraries from the requirements.txt file\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚡ Note: The datasets are too large to upload directly to GitHub, hence manual placement is required.\n",
    "\n",
    "# 📂 Dataset Placement Instructions\n",
    "\n",
    "Please download the following datasets from the [M5 Forecasting Accuracy Kaggle competition](https://www.kaggle.com/competitions/m5-forecasting-accuracy/data):\n",
    "\n",
    "1. `sales_train_evaluation.csv`\n",
    "2. `sales_train_validation.csv`\n",
    "\n",
    "After downloading, **place them in the following directory structure** relative to this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🛠️ Sales Data Reshaping: Wide to Long Format\n",
    "\n",
    "The **sales dataset** provided is originally in a **wide format**, where each day's sales are represented as separate columns.\n",
    "\n",
    "For effective **time-series analysis** and **modeling**, it is essential to reshape this data into a **long format** — where each row represents a single product's sales on a specific day.\n",
    "\n",
    "This preprocessing step **transforms** the dataset and **stores** the reshaped version, enabling easier feature engineering, model training, and forecasting tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : Partitions :  1\n",
      "2 : re-Partitions :  3049\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 53\u001b[0m\n\u001b[0;32m     49\u001b[0m     data_dimentionality_prep(file_read_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/raw/sales_train_validation.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, file_write_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/processed/sales_train_validation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mprocessed\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msales_train_evaluation\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 53\u001b[0m     \u001b[43mdata_dimentionality_prep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_read_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/raw/sales_train_evaluation.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_write_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/processed/sales_train_evaluation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 20\u001b[0m, in \u001b[0;36mdata_dimentionality_prep\u001b[1;34m(file_read_path, file_write_path)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Melt the data: Convert wide format to long format\u001b[39;00m\n\u001b[0;32m     15\u001b[0m sales_data_prep \u001b[38;5;241m=\u001b[39m sales_data_prep\u001b[38;5;241m.\u001b[39mmelt(\n\u001b[0;32m     16\u001b[0m     id_vars\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitem_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdept_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_id\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     17\u001b[0m     var_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     18\u001b[0m     value_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n\u001b[1;32m---> 20\u001b[0m \u001b[43msales_data_prep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Extract day number from column names safely using Pandas\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_day\u001b[39m(df):\n",
      "File \u001b[1;32mc:\\Users\\saila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:713\u001b[0m, in \u001b[0;36mFrameBase.head\u001b[1;34m(self, n, npartitions, compute)\u001b[0m\n\u001b[0;32m    711\u001b[0m out \u001b[38;5;241m=\u001b[39m new_collection(expr\u001b[38;5;241m.\u001b[39mHead(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39mn, npartitions\u001b[38;5;241m=\u001b[39mnpartitions))\n\u001b[0;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[1;32m--> 713\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\saila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\dataframe\\dask_expr\\_collection.py:491\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[1;34m(self, fuse, concatenate, **kwargs)\u001b[0m\n\u001b[0;32m    489\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    490\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[1;32m--> 491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mcompute(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\saila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\base.py:370\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    348\u001b[0m \n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 370\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\saila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\dask\\base.py:656\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    653\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    655\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 656\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\saila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[0;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[1;32mc:\\Users\\saila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## The Sales dataset is in wide format, we need it in long format, this operation preprocess it and stores in long format\n",
    "\n",
    "\n",
    "def data_dimentionality_prep(file_read_path, file_write_path):\n",
    "    sales_data_prep = dd.read_csv(file_read_path)\n",
    "    print(\"1 : Partitions : \", sales_data_prep.npartitions)\n",
    "    groups = sales_data_prep['item_id'].unique().compute().sort_values().values.tolist()\n",
    "    groups.append(groups[-1])\n",
    "    sales_data_prep = sales_data_prep.set_index('item_id',\n",
    "                        divisions=groups\n",
    "                        ).reset_index()\n",
    "    print(\"2 : re-Partitions : \", sales_data_prep.npartitions)\n",
    "\n",
    "    # Melt the data: Convert wide format to long format\n",
    "    sales_data_prep = sales_data_prep.melt(\n",
    "        id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    "        var_name=\"day\",\n",
    "        value_name=\"sales\"\n",
    "    )\n",
    "    sales_data_prep.head()\n",
    "\n",
    "    # Extract day number from column names safely using Pandas\n",
    "    def extract_day(df):\n",
    "        df[\"day\"] = df[\"day\"].str.extract(r\"d_(\\d+)\").astype(int)\n",
    "        return df\n",
    "\n",
    "    meta = {\n",
    "        \"id\": \"object\",\n",
    "        \"item_id\": \"object\",\n",
    "        \"dept_id\": \"object\",\n",
    "        \"cat_id\": \"object\",\n",
    "        \"store_id\": \"object\",\n",
    "        \"state_id\": \"object\",\n",
    "        \"day\": \"int64\",\n",
    "        \"sales\": \"float64\"\n",
    "    }\n",
    "\n",
    "    # Apply transformation using map_partitions\n",
    "    sales_data_prep = sales_data_prep.map_partitions(extract_day, meta=meta)\n",
    "    sales_data_prep = sales_data_prep.map_partitions(lambda df: df.sort_values(['id',\"day\"]))\n",
    "\n",
    "    print(\"3 : final Partitions : \", sales_data_prep.npartitions)\n",
    "    # Save processed sales data\n",
    "    sales_data_prep.to_parquet(file_write_path)\n",
    "\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "if not os.path.isdir('..\\data\\processed\\sales_train_validation'):\n",
    "    data_dimentionality_prep(file_read_path = '../data/raw/sales_train_validation.csv', file_write_path = '../data/processed/sales_train_validation')\n",
    "    \n",
    "\n",
    "if not os.path.isdir('..\\data\\processed\\sales_train_evaluation'):\n",
    "    data_dimentionality_prep(file_read_path = '../data/raw/sales_train_evaluation.csv', file_write_path = '../data/processed/sales_train_evaluation')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset efficiently using Dask\n",
    "\n",
    "prices_data = dd.read_csv('..\\dataset\\M5 forecasting accuracy\\sell_prices.csv')\n",
    "calendar_data = dd.read_csv('..\\dataset\\M5 forecasting accuracy\\calendar.csv', dtype={\n",
    "        'event_name_1': 'object',\n",
    "        'event_type_1': 'object',\n",
    "        'event_name_2': 'object',\n",
    "        'event_type_2': 'object'\n",
    "    },\n",
    "    assume_missing=True  # Ensures proper dtype handling for missing values\n",
    ")\n",
    "sales_data_val = dd.read_parquet('../data/processed/sales_train_validation')\n",
    "sales_data_eval = dd.read_parquet('../data/processed/sales_train_evaluation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_engineering(data_df):\n",
    "    # Step 2: Handle missing values & outliers\n",
    "    data_df, data_df_missing = handle_missing_values(data_df)\n",
    "\n",
    "    data_df = data_df.map_partitions(\n",
    "        lambda df: df.sort_values([\"id\", \"day\"])\n",
    "    )\n",
    "    data_df = data_df.set_index(\"id\", sorted=True)\n",
    "\n",
    "    # Step 3: Feature Engineering\n",
    "    # Creating lag features for sales\n",
    "    data_df = create_lag_features(data_df, lags=[1, 7, 28])\n",
    "\n",
    "    # Rolling window features\n",
    "    data_df = create_rolling_features(data_df)\n",
    "\n",
    "    \n",
    "    return data_df, data_df_missing\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "prices_data, prices_missing = handle_missing_values(prices_data)\n",
    "calendar_data, calendar_missing = handle_missing_values(calendar_data)\n",
    "\n",
    "sales_val, sales_missing = handle_missing_values(sales_data_val)\n",
    "sales_eval, _ = handle_missing_values(sales_data_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data\n",
    "# Export cleaned and engineered dataset\n",
    "sales_val.to_parquet(\"./data/final_sales_data_val\")\n",
    "sales_eval.to_parquet(\"./data/final_sales_data_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data engineering completed. Processed data saved.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Generate Data Quality Report\n",
    "quality_report = {\n",
    "    'sales_missing': sales_missing.to_dict(),\n",
    "    'prices_missing': prices_missing.to_dict(),\n",
    "    'calendar_missing': calendar_missing.to_dict()\n",
    "}\n",
    "\n",
    "with open('./data/data_quality_report.json', 'w') as f:\n",
    "    json.dump(quality_report, f, indent=4)\n",
    "\n",
    "\n",
    "print(\"Data engineering completed. Processed data saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
