{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è M5 Forecasting Data Preprocessing and Feature Engineering\n",
    "\n",
    "This notebook focuses on efficiently preparing the **M5 Forecasting** dataset for further analysis and modeling, primarily using **Dask** for handling large data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. üì¶ Import Required Libraries\n",
    "- Imported Dask, NumPy, Pandas, OS, and JSON libraries.\n",
    "- Note: All necessary libraries should be installed from the provided `requirements.txt`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. üìÇ Create Output Directory\n",
    "- Created a `./data` folder if it doesn't already exist to store processed datasets.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 3. üõí Sales Data Preprocessing\n",
    "- Read `sales_train_validation.csv` using Dask.\n",
    "- **Truncated** the dataset to only include `HOBBIES` category products due to limited compute resources.\n",
    "- **Repartitioned** the dataset based on `item_id` for better parallelism.\n",
    "- **Melted** the data:\n",
    "  - Converted it from wide format (columns `d_1`, `d_2`, ...) to long format (one row per item-day pair).\n",
    "- Extracted day numbers (e.g., `d_1` ‚Üí `1`) from the `day` column.\n",
    "- **Sorted** data by `id` and `day`.\n",
    "- Saved the processed sales data in **Parquet** format for faster loading.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. üìà Load Datasets\n",
    "- Loaded the following datasets:\n",
    "  - **Sell prices**: `sell_prices.csv`\n",
    "  - **Calendar**: `calendar.csv` (handled categorical columns and missing values properly)\n",
    "  - **Sales**: Loaded preprocessed sales data from the saved parquet files.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. üßπ Handling Missing Values\n",
    "- Created a utility function `handle_missing_values(df)`:\n",
    "  - Generated a missing value report (percentage of nulls per column).\n",
    "  - Imputed missing values using **forward fill** (`ffill`) followed by **backward fill** (`bfill`).\n",
    "- Applied this function across **sales**, **prices**, and **calendar** datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. üõ†Ô∏è Memory Management\n",
    "- Repartitioned the sales dataset for better memory management and processing efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. üß™ Feature Engineering\n",
    "- **Sorted** sales data by `id` and `day` again to ensure correct sequencing.\n",
    "- **Set `id` as the index** (ensuring a sorted index for partition operations).\n",
    "- Created **lag features**:\n",
    "  - Added lagged sales columns for **1**, **7**, and **28** days.\n",
    "  - These features will help capture sales patterns and temporal dependencies during modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Output:\n",
    "- Preprocessed sales data with lag features ready for modeling.\n",
    "- Missing values handled across all datasets.\n",
    "- All transformations are optimized for large-scale data handling using Dask.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\cupy\\_environment.py:217: UserWarning: CUDA path could not be detected. Set CUDA_PATH environment variable if CuPy fails to load.\n",
      "  warnings.warn(\n",
      "c:\\Users\\saila\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Importing required libraries\n",
    "#Note: Install all libraries from the requirements.txt file\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö° Note: The datasets are too large to upload directly to GitHub, hence manual placement is required.\n",
    "\n",
    "# üìÇ Dataset Placement Instructions\n",
    "\n",
    "Please download the following datasets from the [M5 Forecasting Accuracy Kaggle competition](https://www.kaggle.com/competitions/m5-forecasting-accuracy/data):\n",
    "\n",
    "1. `sales_train_evaluation.csv`\n",
    "2. `sales_train_validation.csv`\n",
    "\n",
    "After downloading, **place them in the following directory structure** relative to this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(\"./data\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üõ†Ô∏è Sales Data Reshaping: Wide to Long Format\n",
    "\n",
    "The **sales dataset** provided is originally in a **wide format**, where each day's sales are represented as separate columns.\n",
    "\n",
    "For effective **time-series analysis** and **modeling**, it is essential to reshape this data into a **long format** ‚Äî where each row represents a single product's sales on a specific day.\n",
    "\n",
    "This preprocessing step **transforms** the dataset and **stores** the reshaped version, enabling easier feature engineering, model training, and forecasting tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Sales dataset is in wide format, we need it in long format, this operation preprocess it and stores in long format\n",
    "if not os.path.isdir('./data/processed_sales_data'):\n",
    "    sales_data_prep = dd.read_csv('.\\dataset\\M5 forecasting accuracy\\sales_train_validation.csv')\n",
    "\n",
    "    ## For this project I am not able to efford the compute, so truncating the dataset\n",
    "    sales_data_prep = sales_data_prep[sales_data_prep[\"cat_id\"] == \"HOBBIES\"]\n",
    "\n",
    "    print(\"1 : Partitions : \", sales_data_prep.npartitions)\n",
    "    groups = sales_data_prep['item_id'].unique().compute().sort_values().values.tolist()\n",
    "    groups.append(groups[-1])\n",
    "    sales_data_prep = sales_data_prep.set_index('item_id',\n",
    "                        divisions=groups\n",
    "                        ).reset_index()\n",
    "    print(\"2 : re-Partitions : \", sales_data_prep.npartitions)\n",
    "\n",
    "    # Melt the data: Convert wide format to long format\n",
    "    sales_data_prep = sales_data_prep.melt(\n",
    "        id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    "        var_name=\"day\",\n",
    "        value_name=\"sales\"\n",
    "    )\n",
    "    sales_data_prep.head()\n",
    "\n",
    "    # Extract day number from column names safely using Pandas\n",
    "    def extract_day(df):\n",
    "        df[\"day\"] = df[\"day\"].str.extract(r\"d_(\\d+)\").astype(int)\n",
    "        return df\n",
    "\n",
    "    meta = {\n",
    "        \"id\": \"object\",\n",
    "        \"item_id\": \"object\",\n",
    "        \"dept_id\": \"object\",\n",
    "        \"cat_id\": \"object\",\n",
    "        \"store_id\": \"object\",\n",
    "        \"state_id\": \"object\",\n",
    "        \"day\": \"int64\",\n",
    "        \"sales\": \"float64\"\n",
    "    }\n",
    "\n",
    "    # Apply transformation using map_partitions\n",
    "    sales_data_prep = sales_data_prep.map_partitions(extract_day, meta=meta)\n",
    "    sales_data_prep = sales_data_prep.map_partitions(lambda df: df.sort_values(['id',\"day\"]))\n",
    "\n",
    "    print(\"3 : final Partitions : \", sales_data_prep.npartitions)\n",
    "    # Save processed sales data\n",
    "    sales_data_prep.to_parquet(\"./data/processed_sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset efficiently using Dask\n",
    "\n",
    "prices_data = dd.read_csv('.\\dataset\\M5 forecasting accuracy\\sell_prices.csv')\n",
    "calendar_data = dd.read_csv('.\\dataset\\M5 forecasting accuracy\\calendar.csv', dtype={\n",
    "        'event_name_1': 'object',\n",
    "        'event_type_1': 'object',\n",
    "        'event_name_2': 'object',\n",
    "        'event_type_2': 'object'\n",
    "    },\n",
    "    assume_missing=True  # Ensures proper dtype handling for missing values\n",
    ")\n",
    "sales_data = dd.read_parquet(\"./data/processed_sales_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Handle missing values & outliers\n",
    "def handle_missing_values(df):\n",
    "    missing_report = df.isnull().mean().compute()\n",
    "    df = df.ffill().bfill()\n",
    "    return df, missing_report\n",
    "\n",
    "sales_data, sales_missing = handle_missing_values(sales_data)\n",
    "prices_data, prices_missing = handle_missing_values(prices_data)\n",
    "calendar_data, calendar_missing = handle_missing_values(calendar_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions :  565\n"
     ]
    }
   ],
   "source": [
    "#Repartition to manage memory better\n",
    "print(\"Partitions : \", sales_data.npartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Feature Engineering\n",
    "# Creating lag features for sales\n",
    "\n",
    "sales_data = sales_data.map_partitions(\n",
    "    lambda df: df.sort_values([\"id\", \"day\"])\n",
    ")\n",
    "sales_data = sales_data.set_index(\"id\", sorted=True)\n",
    "\n",
    "def add_lag_features(df, lags=[1, 7, 28]):\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df.groupby('id')['sales'].shift(lag)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Create updated meta\n",
    "new_cols = {f'lag_{lag}': 'float64' for lag in [1, 7, 28]}\n",
    "meta = sales_data._meta.assign(**{k: pd.Series(dtype=v) for k,v in new_cols.items()})\n",
    "sales_data = sales_data.map_partitions(add_lag_features, meta=meta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling window features\n",
    "def create_rolling_features(df, window_sizes=[7, 14]):\n",
    "    def rolling_func(partition_df):\n",
    "        partition_df = partition_df.sort_values(['id', 'day'])\n",
    "        for window in window_sizes:\n",
    "            partition_df[f'rolling_mean_{window}'] = (\n",
    "                partition_df.groupby('id')['sales']\n",
    "                            .rolling(window=window, min_periods=1)\n",
    "                            .mean()\n",
    "                            .reset_index(drop=True)\n",
    "            )\n",
    "        return partition_df\n",
    "    \n",
    "    # Create updated meta\n",
    "    new_cols = {f'rolling_mean_{w}': 'float64' for w in window_sizes}\n",
    "    meta = df._meta.assign(**{k: pd.Series(dtype=v) for k,v in new_cols.items()})\n",
    "    df = df.map_partitions(rolling_func, meta=meta)\n",
    "    return df\n",
    "\n",
    "sales_data = create_rolling_features(sales_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data engineering completed. Processed data saved.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Generate Data Quality Report\n",
    "quality_report = {\n",
    "    'sales_missing': sales_missing.to_dict(),\n",
    "    'prices_missing': prices_missing.to_dict(),\n",
    "    'calendar_missing': calendar_missing.to_dict()\n",
    "}\n",
    "\n",
    "with open('./data/data_quality_report.json', 'w') as f:\n",
    "    json.dump(quality_report, f, indent=4)\n",
    "\n",
    "# Save processed data\n",
    "sales_data.to_parquet(\"./data/final_sales_data\")  # Export cleaned and engineered dataset\n",
    "print(\"Data engineering completed. Processed data saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
