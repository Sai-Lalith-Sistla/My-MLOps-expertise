{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è M5 Forecasting Data Preprocessing and Feature Engineering\n",
    "\n",
    "This notebook focuses on efficiently preparing the **M5 Forecasting** dataset for further analysis and modeling, primarily using **Dask** for handling large data.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1. üì¶ Import Required Libraries\n",
    "- Imported Dask, NumPy, Pandas, OS, and JSON libraries.\n",
    "- Note: All necessary libraries should be installed from the provided `requirements.txt`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. üìÇ Create Output Directory\n",
    "- Created a `./data` folder if it doesn't already exist to store processed datasets.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### 3. üõí Sales Data Preprocessing\n",
    "- Read `sales_train_validation.csv` using Dask.\n",
    "- **Truncated** the dataset to only include `HOBBIES` category products due to limited compute resources.\n",
    "- **Repartitioned** the dataset based on `item_id` for better parallelism.\n",
    "- **Melted** the data:\n",
    "  - Converted it from wide format (columns `d_1`, `d_2`, ...) to long format (one row per item-day pair).\n",
    "- Extracted day numbers (e.g., `d_1` ‚Üí `1`) from the `day` column.\n",
    "- **Sorted** data by `id` and `day`.\n",
    "- Saved the processed sales data in **Parquet** format for faster loading.\n",
    "\n",
    "---\n",
    "\n",
    "#### 4. üìà Load Datasets\n",
    "- Loaded the following datasets:\n",
    "  - **Sell prices**: `sell_prices.csv`\n",
    "  - **Calendar**: `calendar.csv` (handled categorical columns and missing values properly)\n",
    "  - **Sales**: Loaded preprocessed sales data from the saved parquet files.\n",
    "\n",
    "---\n",
    "\n",
    "#### 5. üßπ Handling Missing Values\n",
    "- Created a utility function `handle_missing_values(df)`:\n",
    "  - Generated a missing value report (percentage of nulls per column).\n",
    "  - Imputed missing values using **forward fill** (`ffill`) followed by **backward fill** (`bfill`).\n",
    "- Applied this function across **sales**, **prices**, and **calendar** datasets.\n",
    "\n",
    "---\n",
    "\n",
    "#### 6. üõ†Ô∏è Memory Management\n",
    "- Repartitioned the sales dataset for better memory management and processing efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "#### 7. üß™ Feature Engineering\n",
    "- **Sorted** sales data by `id` and `day` again to ensure correct sequencing.\n",
    "- **Set `id` as the index** (ensuring a sorted index for partition operations).\n",
    "- Created **lag features**:\n",
    "  - Added lagged sales columns for **1**, **7**, and **28** days.\n",
    "  - These features will help capture sales patterns and temporal dependencies during modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Output:\n",
    "- Preprocessed sales data with lag features ready for modeling.\n",
    "- Missing values handled across all datasets.\n",
    "- All transformations are optimized for large-scale data handling using Dask.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "#Note: Install all libraries from the requirements.txt file\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö° Note: The datasets are too large to upload directly to GitHub, hence manual placement is required.\n",
    "\n",
    "# üìÇ Dataset Placement Instructions\n",
    "\n",
    "Please download the following datasets from the [M5 Forecasting Accuracy Kaggle competition](https://www.kaggle.com/competitions/m5-forecasting-accuracy/data):\n",
    "\n",
    "1. `sales_train_evaluation.csv`\n",
    "2. `sales_train_validation.csv`\n",
    "\n",
    "After downloading, **place them in the following directory structure** relative to this notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üõ†Ô∏è Sales Data Reshaping: Wide to Long Format\n",
    "\n",
    "The **sales dataset** provided is originally in a **wide format**, where each day's sales are represented as separate columns.\n",
    "\n",
    "For effective **time-series analysis** and **modeling**, it is essential to reshape this data into a **long format** ‚Äî where each row represents a single product's sales on a specific day.\n",
    "\n",
    "This preprocessing step **transforms** the dataset and **stores** the reshaped version, enabling easier feature engineering, model training, and forecasting tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Sales dataset is in wide format, we need it in long format, this operation preprocess it and stores in long format\n",
    "def data_repartition_dask(df, key_col):\n",
    "    groups = df[key_col].unique().compute().sort_values().values.tolist()\n",
    "    groups.append(groups[-1])\n",
    "    df = df.set_index(key_col, divisions=groups ).reset_index()\n",
    "    return df\n",
    "\n",
    "def data_dimentionality_prep(file_read_path, file_write_path):\n",
    "    sales_data_prep = dd.read_csv(file_read_path)\n",
    "    print(\"1 : Partitions : \", sales_data_prep.npartitions)\n",
    "    sales_data_prep = data_repartition_dask(sales_data_prep, 'item_id')\n",
    "    print(\"2 : re-Partitions : \", sales_data_prep.npartitions)\n",
    "\n",
    "    # Melt the data: Convert wide format to long format\n",
    "    sales_data_prep = sales_data_prep.melt(\n",
    "        id_vars=[\"id\", \"item_id\", \"dept_id\", \"cat_id\", \"store_id\", \"state_id\"],\n",
    "        var_name=\"day\",\n",
    "        value_name=\"sales\"\n",
    "    )\n",
    "    sales_data_prep.head()\n",
    "\n",
    "    # Extract day number from column names safely using Pandas\n",
    "    def extract_day(df):\n",
    "        df[\"day\"] = df[\"day\"].str.extract(r\"d_(\\d+)\").astype(int)\n",
    "        return df\n",
    "\n",
    "    meta = {\n",
    "        \"id\": \"object\",\n",
    "        \"item_id\": \"object\",\n",
    "        \"dept_id\": \"object\",\n",
    "        \"cat_id\": \"object\",\n",
    "        \"store_id\": \"object\",\n",
    "        \"state_id\": \"object\",\n",
    "        \"day\": \"int64\",\n",
    "        \"sales\": \"float64\"\n",
    "    }\n",
    "\n",
    "    # Apply transformation using map_partitions\n",
    "    sales_data_prep = sales_data_prep.map_partitions(extract_day, meta=meta)\n",
    "    sales_data_prep = sales_data_prep.map_partitions(lambda df: df.sort_values(['id',\"day\"]))\n",
    "\n",
    "    print(\"3 : final Partitions : \", sales_data_prep.npartitions)\n",
    "    # Save processed sales data\n",
    "    sales_data_prep.to_parquet(file_write_path)\n",
    "\n",
    "\n",
    "# Create output folder if it doesn't exist\n",
    "os.makedirs(\"../data\", exist_ok=True)\n",
    "\n",
    "if not os.path.isdir('..\\data\\processed\\sales_train_validation'):\n",
    "    data_dimentionality_prep(file_read_path = '../data/raw/sales_train_validation.csv', file_write_path = '../data/processed/sales_train_validation')\n",
    "    \n",
    "\n",
    "if not os.path.isdir('..\\data\\processed\\sales_train_evaluation'):\n",
    "    data_dimentionality_prep(file_read_path = '../data/raw/sales_train_evaluation.csv', file_write_path = '../data/processed/sales_train_evaluation')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset efficiently using Dask\n",
    "\n",
    "prices_data = dd.read_csv('../data/raw/sell_prices.csv')\n",
    "calendar_data = dd.read_csv('../data/raw/calendar.csv', dtype={\n",
    "        'event_name_1': 'object',\n",
    "        'event_type_1': 'object',\n",
    "        'event_name_2': 'object',\n",
    "        'event_type_2': 'object'\n",
    "    },\n",
    "    assume_missing=True  # Ensures proper dtype handling for missing values\n",
    ")\n",
    "sales_data_val = dd.read_parquet('../data/processed/sales_train_validation')\n",
    "sales_data_eval = dd.read_parquet('../data/processed/sales_train_evaluation')\n",
    "\n",
    "#Have mercy on my personal compute, truncating data with filters\n",
    "item_ids = [\n",
    "    'HOBBIES_1_001',\n",
    "    'HOBBIES_1_002',\n",
    "    'HOBBIES_1_003',\n",
    "    'HOBBIES_1_004',\n",
    "    'HOBBIES_1_005'\n",
    "]\n",
    "sales_data_val = data_repartition_dask(sales_data_val[sales_data_val['item_id'].isin(item_ids)], 'item_id')\n",
    "sales_data_eval = data_repartition_dask(sales_data_eval[sales_data_eval['item_id'].isin(item_ids)], 'item_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_engineering(data_df):\n",
    "    # Step 2: Handle missing values & outliers\n",
    "    data_df, data_df_missing = handle_missing_values(data_df)\n",
    "\n",
    "    data_df = data_df.map_partitions(\n",
    "        lambda df: df.sort_values([\"id\", \"day\"])\n",
    "    )\n",
    "    data_df = data_df.set_index(\"id\", sorted=True)\n",
    "\n",
    "    # Step 3: Feature Engineering\n",
    "    # Creating lag features for sales\n",
    "    data_df = create_lag_features(data_df, lags=[1, 7, 28])\n",
    "\n",
    "    # Rolling window features\n",
    "    data_df = create_rolling_features(data_df)\n",
    "    data_df = data_df.reset_index()\n",
    "\n",
    "    return data_df, data_df_missing\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_data, prices_missing = handle_missing_values(prices_data)\n",
    "calendar_data, calendar_missing = handle_missing_values(calendar_data)\n",
    "\n",
    "\n",
    "#back to DE \n",
    "sales_val, sales_missing = data_engineering(sales_data_val)\n",
    "sales_eval, _ = data_engineering(sales_data_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label categorical data\n",
    "\n",
    "manager = DaskLabelEncoderManager(columns=['id', 'item_id',\t'dept_id',\t'cat_id',\t'store_id',\t'state_id'])\n",
    "\n",
    "# Train phase\n",
    "sales_val_encoded = manager.fit_transform(sales_val)\n",
    "# sales_val_encoded = dd.from_delayed([sales_val_encoded.compute()])\n",
    "manager.save('../data/encoders.pkl')\n",
    "\n",
    "# # Test phase\n",
    "sales_eval_encoded = manager.transform(sales_eval)\n",
    "\n",
    "# Save processed data\n",
    "# Export cleaned and engineered dataset\n",
    "sales_val_encoded.to_parquet(\"../data/processed/final_sales_data_val\")\n",
    "sales_eval_encoded.to_parquet(\"../data/processed/final_sales_data_eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Generate Data Quality Report\n",
    "# TODO : Extend this very vanila version to profiling, validation rules, Anomaly detection, report generation\n",
    "quality_report = {\n",
    "    'sales_missing': sales_missing.to_dict(),\n",
    "    'prices_missing': prices_missing.to_dict(),\n",
    "    'calendar_missing': calendar_missing.to_dict()\n",
    "}\n",
    "\n",
    "with open('../data/data_quality_report.json', 'w') as f:\n",
    "    json.dump(quality_report, f, indent=4)\n",
    "\n",
    "\n",
    "print(\"Data engineering completed. Processed data saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
